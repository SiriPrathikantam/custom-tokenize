# ðŸ”¤ Custom Tokenizer with Python

A beginner-friendly project to build your own tokenizer â€” like how GPT models break down text into tokens.

---

## ðŸ“¦ What's Inside?

### âœ… Version 1 â€“ Basic Tokenizer
- Tokenizes text (words + punctuation)
- Builds a vocabulary from `harrypotter.txt`
- Converts text to token IDs and back
- Handles unknown tokens with `<UNK>`

### âœ… Version 2 â€“ Advanced Tokenizer
- Tracks unknown tokens with positions
- Shows `<UNK:actual_word>` during decoding
- Saves unknown tokens to a log file
- Auto-creates logs folder with timestamps

---

## ðŸ§± Folder Structure

LLM/
â”œâ”€â”€ harrypotter.txt # Sample text for vocab
â”œâ”€â”€ tokenizer_v1/ # Version 1 code
â”œâ”€â”€ tokenizer_v2/ # Version 2 code + logs
â”œâ”€â”€ utils/ # Vocabulary generator
â”œâ”€â”€ scratch/ # Rough work / sandbox
â””â”€â”€ README.md # This file